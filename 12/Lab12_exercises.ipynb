{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME IMPORTS\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import multiprocessing\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET SOME ENVIRONMENTAL VARIABLES\n",
    "os.environ['PYSPARK_PYTHON']=\"C:\\\\Users\\\\kinkr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"C:\\\\Users\\\\kinkr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\python.exe\"\n",
    "os.environ['SPARK_LOCAL_HOSTNAME']=\"localhost\"\n",
    "os.environ['SPARK_HOME']=\"C:\\\\Users\\\\kinkr\\\\Desktop\\\\DyskD\\\\Pooolibuda\\\\VIsemestr\\\\IR\\\\Laby\\\\12\\\\spark-2.2.1-bin-hadoop2.7\"\n",
    "os.environ['JAVA_HOME']=\"C:\\\\Users\\\\kinkr\\\\Downloads\\\\jdk1.8.0_231\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF FINDSPARK WORKS CORRECTLY\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START SPARK CONTEXT ON LOCAL MACHINE\n",
    "sc = SparkContext(\"local\", appName=\"Test\")\n",
    "##------------------------------------\n",
    "# GO TO LOCALHOST:4040 and ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP SPARK CONTEXT\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of logical CPUs is 12\n"
     ]
    }
   ],
   "source": [
    "# OBTAIN THE NUMBER OF LOGICAL CPUs\n",
    "cpus = multiprocessing.cpu_count()\n",
    "print(\"The number of logical CPUs is \" + str(cpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Compute the value of PI using Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is solved. Your task is to read and analyse the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method generates one sample point and verifies whether it is inside a circle or not.\n",
    "# The input is passed via filter method, however, we do not need it here\n",
    "def inside(inValue):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method estimates the value of PI\n",
    "def computePI_MonteCarlo_v1(sc, samples, partitions):\n",
    "    # Create Resilient Distributed Dataset (RDDs) containing SAMPLES elements.\n",
    "    # This data is distributed (parallelized) among available nodes (here, CPUs - partitions).\n",
    "    dff = sc.parallelize(range(0, samples), partitions)\n",
    "    # Filter out these samples that are not inside a circle.\n",
    "    # For this purpose, Inside method is run and returns\n",
    "    # true/false (for each data element) with appropriate probability distribution\n",
    "    # Why do we generate samples \"on fly\"?\n",
    "    filtered = dff.filter(inside)\n",
    "    # count the number of hits\n",
    "    left = filtered.count()\n",
    "    # Estimate the value of PI and return it\n",
    "    return 4.0 * float(left) / float(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo simulation for 10000000 samples\n",
      "True value of PI = 3.1415926535...\n",
      "  Number of CPUs = 1 | Time = 14.2391 s | Result(PI) = 3.14126640\n",
      "  Number of CPUs = 2 | Time = 12.0808 s | Result(PI) = 3.14217000\n",
      "  Number of CPUs = 3 | Time = 9.1784 s | Result(PI) = 3.14288760\n",
      "  Number of CPUs = 4 | Time = 9.8604 s | Result(PI) = 3.14130200\n",
      "  Number of CPUs = 5 | Time = 10.1567 s | Result(PI) = 3.14069400\n",
      "  Number of CPUs = 6 | Time = 10.9956 s | Result(PI) = 3.14141680\n",
      "  Number of CPUs = 7 | Time = 11.0916 s | Result(PI) = 3.14125880\n",
      "  Number of CPUs = 8 | Time = 12.1290 s | Result(PI) = 3.14199040\n",
      "  Number of CPUs = 9 | Time = 12.7821 s | Result(PI) = 3.14235000\n",
      "  Number of CPUs = 10 | Time = 8.9624 s | Result(PI) = 3.14091720\n",
      "  Number of CPUs = 11 | Time = 12.8164 s | Result(PI) = 3.14209360\n",
      "  Number of CPUs = 12 | Time = 14.8993 s | Result(PI) = 3.14243240\n"
     ]
    }
   ],
   "source": [
    "### ESTIMATE VALUE OF PI \n",
    "samples = 10000000\n",
    "\n",
    "print(\"Monte Carlo simulation for \" + str(samples) + \" samples\")\n",
    "print(\"True value of PI = 3.1415926535...\")\n",
    "\n",
    "## i = number of nodes (CPUs)\n",
    "for i in range(1, cpus + 1):\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"PI_MonteCarlo\")\n",
    "    start_time = time.time()\n",
    "    piValue = computePI_MonteCarlo_v1(sc, samples, i)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"  Number of CPUs = %i | Time = %.4f s | Result(PI) = %.8f\" % (i, elapsed, piValue))  \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 2: Wordcount //all done but needs check - check comments!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy collection 1: 3 short documents\n",
    "# create RDD divided into n-paritions\n",
    "def getSmallCollection_EX1(sc, partitions):\n",
    "    doc1 = \"Roses,are red \"\n",
    "    doc2 = \"Roses are roses\"\n",
    "    doc3 = \"The Sun is red.\"\n",
    "    rdd1 = sc.parallelize([doc1, doc2, doc3], partitions)\n",
    "    return rdd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Dummy collection 2: ~200 documents about animals (ant.html, dog.html, panda.html, hedgehog.html, etc.). For this purpose, download www.cs.put.poznan.pl/mtomczyk/ir/lab6/pages.zip, unzip, and copy \"pages\" folder into your working directory. //I THINK IT SHOULDN'T BE MODIFIED BUT I HAD TO ADD * TO PATH TO WORK FINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargeCollection_EX1(sc, partitions):\n",
    "    DOCS = sc.wholeTextFiles(\"./pages/*\", partitions)\n",
    "    rdd1 = DOCS.map(lambda x: x[1])\n",
    "    return rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given text \"x\", this method performs simple tokenization and normalization (returns a list of terms)\n",
    "def tokenizeAndNormalize(x):\n",
    "    return [s.lower() for s in re.split(' |;|,|\\t|\\n|\\.', x) if len(s) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Init spark context (1 core):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[1]\", appName=\"Word_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) TODO: Collect the data (getSmallCollection_EX1): //1 partition??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Roses,are red ', 'Roses are roses', 'The Sun is red.']\n"
     ]
    }
   ],
   "source": [
    "rdd1 = getSmallCollection_EX1(sc, 1)\n",
    "# if you whish to print data stored in rdd, use print(rdd.collect())\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) TODO: Firslty, you should tokenize all documents. For this purpose use flatMap function (rdd2 = rdd1.flatMap) where you pass tokenizeAndNormalize method. There are two methods: map and flatMap. Both produce an output for each element of RDD object. The difference is that map keeps produced elements organised and flatMap puts them into a single list, e.g.: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 3)]\n",
      "['a', 2, 'b', 3]\n"
     ]
    }
   ],
   "source": [
    "tempRDD = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "print(tempRDD.map(lambda x: (x[0], x[1]+1)).collect())\n",
    "print(tempRDD.flatMap(lambda x: (x[0], x[1]+1)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roses', 'are', 'red', 'roses', 'are', 'roses', 'the', 'sun', 'is', 'red']\n"
     ]
    }
   ],
   "source": [
    "# Complete the task here (flatMap with tokenizeAndNormalize):\n",
    "rdd2 = rdd1.flatMap(tokenizeAndNormalize)\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) TODO: Now for each term produce (term, 1). Use map (why not flatMap?) with lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 1), ('are', 1), ('red', 1), ('roses', 1), ('are', 1), ('roses', 1), ('the', 1), ('sun', 1), ('is', 1), ('red', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x, 1))\n",
    "print(rdd3.collect())\n",
    "# ANSWER\n",
    "# Because we want to keep \"1\" with the word that we obtained from rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) TODO: Now it is time to group the results. Use groupByKey method. When any \"...byKey\" method is invoked, the first element of a stored object is treated as a key. When invoking this method, you should also invoke .mapValues(list) so that all corresponding values will be stored in a single list. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', [1, 1])]\n"
     ]
    }
   ],
   "source": [
    "tempRDD = sc.parallelize([(\"a\", 1), (\"a\", 1)])\n",
    "print(tempRDD.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', [1, 1, 1]), ('are', [1, 1]), ('red', [1, 1]), ('the', [1]), ('sun', [1]), ('is', [1])]\n"
     ]
    }
   ],
   "source": [
    "# Complete the task here:\n",
    "rdd4 = rdd3.groupByKey().mapValues(list)\n",
    "print(rdd4.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) TODO: Now you could use countByKey method but it returns a dictionarty. Use map function again to sum the elements of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 3), ('are', 2), ('red', 2), ('the', 1), ('sun', 1), ('is', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd4.map(lambda x: (x[0], sum(x[1])))\n",
    "print(rdd5.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) TODO: It is almost done but we wish the objects to be sorted (alphabetically). You can use sortByKey method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('are', 2), ('is', 1), ('red', 2), ('roses', 3), ('sun', 1), ('the', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd6 = rdd5.sortByKey()\n",
    "print(rdd6.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) TODO: Done. Bout it could be done in another way. Instead of grouping by key (rdd4) and counting the number of \"1\"s (rdd5), you could use reduceByKey method. reduceByKey \"merges\" all object with the same key. Similar to groupByKey, however, instead of grouping, a new value is computed by provided function, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 4), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "tempRDD = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "print(tempRDD.reduceByKey(lambda x, y: x + y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 3), ('are', 2), ('red', 2), ('the', 1), ('sun', 1), ('is', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Complete the task here. Use rdd3 object to compute rdd7.\n",
    "rdd7 = rdd3.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd7.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) TODO: Sort the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('are', 2), ('is', 1), ('red', 2), ('roses', 3), ('sun', 1), ('the', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd8 = rdd7.sortByKey()\n",
    "print(rdd8.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) TODO: Complete the method doWordCount (just copy your code, use groupByKey + map(sum) version; should return last rdd object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doWordCount(sc, collection, partitions):\n",
    "    # rdd1 data = collection\n",
    "    rdd1 = collection\n",
    "    # rdd2 flatMap\n",
    "    rdd2 = rdd1.flatMap(tokenizeAndNormalize)\n",
    "    # rdd3 map\n",
    "    rdd3 = rdd2.map(lambda x: (x, 1))\n",
    "    # rdd4 groupByKey\n",
    "    rdd4 = rdd3.groupByKey().mapValues(list)\n",
    "    # rdd5 map (sum)\n",
    "    rdd5 = rdd4.map(lambda x: (x[0], sum(x[1])))\n",
    "    # rdd6 sortByKey\n",
    "    rdd6 = rdd5.sortByKey()\n",
    "    return rdd6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) TODO: Run the script and observe the results (why is the best time for 1CPU?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 0.0713 s \n",
      "Number of CPUs = 2 | Time = 9.2724 s \n",
      "Number of CPUs = 3 | Time = 12.3559 s \n",
      "Number of CPUs = 4 | Time = 14.2472 s \n",
      "Number of CPUs = 5 | Time = 17.5708 s \n",
      "Number of CPUs = 6 | Time = 20.6613 s \n",
      "Number of CPUs = 7 | Time = 21.2447 s \n",
      "Number of CPUs = 8 | Time = 24.3302 s \n",
      "Number of CPUs = 9 | Time = 26.7188 s \n",
      "Number of CPUs = 10 | Time = 30.5446 s \n",
      "Number of CPUs = 11 | Time = 33.9432 s \n",
      "Number of CPUs = 12 | Time = 34.2755 s \n"
     ]
    }
   ],
   "source": [
    "## i = number of nodes (CPUs). \n",
    "for i in range(1, cpus + 1):\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"WordCount\")\n",
    "    start_time = time.time()\n",
    "    rdd1 = getSmallCollection_EX1(sc, i)\n",
    "    computedData = doWordCount(sc, rdd1, i)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    sc.stop()\n",
    "    \n",
    "# ANSWER\n",
    "# we are doing two things to compute results: getSmallCollection_EX1 and doWordCount, which for our dummy collection\n",
    "# aren't very demanding, so we don't need muplitple CPU's to spread the work among them, one is fine. So because we are\n",
    "# telling multiple CPUs to do the job, the also need to communicate with each other, which means they need to \n",
    "# synchronize and use shared memory and so on. That causes longer computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) TODO: Modyfy the above script (work on a copy, use the cell below) so that the top 3 most common words are printed. Use 1-2CPUs. computedData is an RDD object so you can use sortBy function to resort the elements. // I did nothing here should I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 0.0919 s \n",
      "   0 : 'roses' occured 3 times\n",
      "   1 : 'are' occured 2 times\n",
      "   2 : 'red' occured 2 times\n",
      "Number of CPUs = 2 | Time = 19.4137 s \n",
      "   0 : 'roses' occured 3 times\n",
      "   1 : 'are' occured 2 times\n",
      "   2 : 'red' occured 2 times\n"
     ]
    }
   ],
   "source": [
    "# do the task here\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"WordCount\")\n",
    "    start_time = time.time()\n",
    "    rdd1 = getSmallCollection_EX1(sc, i)\n",
    "    computedData = doWordCount(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 3): #print top 3\n",
    "        print(\"   %i : '%s' occured %d times\" % (i, sortedData[i][0], sortedData[i][1]))\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) TODO: Repeat the experiment for 1-2CPUs and for 2nd collection (much larger). Compare computation times and print the top 20 most common words. Are the results (the most frequent words) similar to the list of english stop words? Why is the difference in time not as big as in \"PI\" example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 1.0096 s \n",
      "   0 : 'the' occured 3027 times\n",
      "   1 : 'and' occured 1910 times\n",
      "   2 : 'of' occured 1553 times\n",
      "   3 : 'in' occured 1165 times\n",
      "   4 : 'are' occured 1031 times\n",
      "   5 : 'to' occured 962 times\n",
      "   6 : 'a' occured 769 times\n",
      "   7 : 'is' occured 622 times\n",
      "   8 : 'as' occured 560 times\n",
      "   9 : 'species' occured 558 times\n",
      "   10 : 'they' occured 370 times\n",
      "   11 : 'for' occured 362 times\n",
      "   12 : 'with' occured 352 times\n",
      "   13 : 'have' occured 344 times\n",
      "   14 : 'their' occured 326 times\n",
      "   15 : 'or' occured 306 times\n",
      "   16 : 'from' occured 269 times\n",
      "   17 : 'by' occured 244 times\n",
      "   18 : 'on' occured 230 times\n",
      "   19 : 'which' occured 214 times\n",
      "Number of CPUs = 2 | Time = 22.9398 s \n",
      "   0 : 'the' occured 3027 times\n",
      "   1 : 'and' occured 1910 times\n",
      "   2 : 'of' occured 1553 times\n",
      "   3 : 'in' occured 1165 times\n",
      "   4 : 'are' occured 1031 times\n",
      "   5 : 'to' occured 962 times\n",
      "   6 : 'a' occured 769 times\n",
      "   7 : 'is' occured 622 times\n",
      "   8 : 'as' occured 560 times\n",
      "   9 : 'species' occured 558 times\n",
      "   10 : 'they' occured 370 times\n",
      "   11 : 'for' occured 362 times\n",
      "   12 : 'with' occured 352 times\n",
      "   13 : 'have' occured 344 times\n",
      "   14 : 'their' occured 326 times\n",
      "   15 : 'or' occured 306 times\n",
      "   16 : 'from' occured 269 times\n",
      "   17 : 'by' occured 244 times\n",
      "   18 : 'on' occured 230 times\n",
      "   19 : 'which' occured 214 times\n"
     ]
    }
   ],
   "source": [
    "# do the task here\n",
    "sc.stop()\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"WordCount\")\n",
    "    start_time = time.time()\n",
    "    rdd1 = getLargeCollection_EX1(sc, i)\n",
    "    computedData = doWordCount(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    sortedData = rddSort.collect()\n",
    "    ### PRINT HERE \n",
    "    for i in range(0, 20):\n",
    "        print(\"   %i : '%s' occured %d times\" % (i, sortedData[i][0], sortedData[i][1]))\n",
    "    ###\n",
    "    sc.stop()\n",
    "### NOT SURE HERE\n",
    "# ANSWERS\n",
    "# yes, the results are similar to stop words. The time difference is bigger here because here we have bigger data sizes\n",
    "# in pi we have single ints and here we have lots of characters in lots of documents which have to be transported across\n",
    "# multi CPUs, and that costs a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Inverted Index + Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are asked to construct inverted index in the following form: (term, the number of doccuments in which the term occurs , sorted list of docIDs]. For instance: [...,(\"roses\", 2, [0, 1]),...] -> term \"roses\" occurs in two documents: termIDs = 0 and 1. The \"get...Collection\" methods are slightly modified. Both return: rdd object, list of the names of the documents, and a dictionary (docID -> document name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSmallCollection_EX2(sc, partitions):\n",
    "    doc1 = \"Roses,are red \"\n",
    "    doc2 = \"Roses are roses\"\n",
    "    doc3 = \"The Sun in red.\"\n",
    "    rdd1 = sc.parallelize([doc1, doc2, doc3], partitions)\n",
    "    docNames = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "    docIDs = {0: docNames[0], 1: docNames[1], 2: docNames[2]}\n",
    "    return rdd1, docNames, docIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargeCollection_EX2(sc, partitions):\n",
    "    DOCS = sc.wholeTextFiles(\"./pages/*\", partitions)\n",
    "    rdd1 = DOCS.map(lambda x: x[1])\n",
    "    rdd2 = DOCS.map(lambda x: x[0])\n",
    "    docNames = rdd2.collect()\n",
    "    docIDs = [i for i in range(0, len(docNames))]\n",
    "    return rdd1, docNames, docIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: do the task and verify the results using the small collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 2, [0, 1]), ('red', 2, [0, 2]), ('are', 2, [0, 1]), ('in', 1, [2]), ('the', 1, [2]), ('sun', 1, [2])]\n"
     ]
    }
   ],
   "source": [
    "def doInvertedIndex(sc, collection, partitions):\n",
    "    rdd1 = collection\n",
    "    rdd2 = rdd1.zipWithIndex()\n",
    "    rdd3 = rdd2.flatMap(lambda x: list(set([(word, x[1]) for word in tokenizeAndNormalize(x[0])])))\n",
    "    rdd4 = rdd3.groupByKey().mapValues(list)\n",
    "    rdd5 = rdd4.map(lambda x: (x[0], len(x[1]), x[1]))\n",
    "    return rdd5\n",
    "sc.stop()\n",
    "master = \"local[\"+str(1)+\"]\" \n",
    "sc = SparkContext(master, appName=\"InvertedIndex\")\n",
    "rdd1, docNames, docIDs = getSmallCollection_EX2(sc, 1)\n",
    "computedData = doInvertedIndex(sc, rdd1, 1)\n",
    "print(computedData.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12) Run the following script and verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 0.0301 s \n",
      "   0 : 'roses' occured in 2 documents\n",
      "   1 : 'red' occured in 2 documents\n",
      "   2 : 'are' occured in 2 documents\n",
      "   3 : 'in' occured in 1 documents\n",
      "   4 : 'the' occured in 1 documents\n",
      "Number of CPUs = 2 | Time = 12.5887 s \n",
      "   0 : 'roses' occured in 2 documents\n",
      "   1 : 'are' occured in 2 documents\n",
      "   2 : 'red' occured in 2 documents\n",
      "   3 : 'in' occured in 1 documents\n",
      "   4 : 'sun' occured in 1 documents\n"
     ]
    }
   ],
   "source": [
    "## i = number of nodes (CPUs). \n",
    "#Why the best time is for 1CPU???\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"InvertedIndex\")\n",
    "    start_time = time.time()\n",
    "    rdd1, docNames, docIDs = getSmallCollection_EX2(sc, i)\n",
    "    computedData = doInvertedIndex(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 5): #print top 3\n",
    "        print(\"   %i : '%s' occured in %i documents\" % (i, sortedData[i][0], sortedData[i][1]))\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Run the following script and verify if it is faster for 2 cores. Lastly, compare the obtained results with the results of exercise 2 (word count). Are the rankings corellated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 10.4339 s \n",
      "   0 : 'the' occured in 206 documents\n",
      "   1 : 'of' occured in 204 documents\n",
      "   2 : 'and' occured in 199 documents\n",
      "   3 : 'in' occured in 194 documents\n",
      "   4 : 'a' occured in 189 documents\n",
      "   5 : 'are' occured in 189 documents\n",
      "   6 : 'to' occured in 187 documents\n",
      "   7 : 'is' occured in 180 documents\n",
      "   8 : 'species' occured in 179 documents\n",
      "   9 : 'as' occured in 155 documents\n",
      "   10 : 'with' occured in 143 documents\n",
      "   11 : 'for' occured in 141 documents\n",
      "   12 : 'or' occured in 138 documents\n",
      "   13 : 'they' occured in 131 documents\n",
      "   14 : 'from' occured in 128 documents\n",
      "   15 : 'their' occured in 127 documents\n",
      "   16 : 'have' occured in 123 documents\n",
      "   17 : 'which' occured in 116 documents\n",
      "   18 : 'family' occured in 116 documents\n",
      "   19 : 'by' occured in 115 documents\n",
      "Number of CPUs = 2 | Time = 30.9393 s \n",
      "   0 : 'the' occured in 206 documents\n",
      "   1 : 'of' occured in 204 documents\n",
      "   2 : 'and' occured in 199 documents\n",
      "   3 : 'in' occured in 194 documents\n",
      "   4 : 'are' occured in 189 documents\n",
      "   5 : 'a' occured in 189 documents\n",
      "   6 : 'to' occured in 187 documents\n",
      "   7 : 'is' occured in 180 documents\n",
      "   8 : 'species' occured in 179 documents\n",
      "   9 : 'as' occured in 155 documents\n",
      "   10 : 'with' occured in 143 documents\n",
      "   11 : 'for' occured in 141 documents\n",
      "   12 : 'or' occured in 138 documents\n",
      "   13 : 'they' occured in 131 documents\n",
      "   14 : 'from' occured in 128 documents\n",
      "   15 : 'their' occured in 127 documents\n",
      "   16 : 'have' occured in 123 documents\n",
      "   17 : 'family' occured in 116 documents\n",
      "   18 : 'which' occured in 116 documents\n",
      "   19 : 'by' occured in 115 documents\n"
     ]
    }
   ],
   "source": [
    "## i = number of nodes (CPUs). \n",
    "#Why the best time is for 1CPU???\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"InvertedIndex\")\n",
    "    start_time = time.time()\n",
    "    rdd1, docNames, docIDs = getLargeCollection_EX2(sc, i)\n",
    "    computedData = doInvertedIndex(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 20): #print top 3\n",
    "        print(\"   %i : '%s' occured in %i documents\" % (i, sortedData[i][0], sortedData[i][1]))\n",
    "    ###\n",
    "    sc.stop()\n",
    "# ANSWERS\n",
    "# Rankings are highly correlated, mainly because of stop words\n",
    "# It's the same reason as before, we have to maintain big sizes of data which can be processed fast enough by one CPU\n",
    "# Adding another CPU results in longer time, because of sharing data between these two."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
